---
title: "Week 8 Lab - GitHub Tutorial"
author: "Craig"
date: "March 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(sf)
library(tmap)
library(leaflet)
library(spatstat)
library(maptools)



```

##Column Graph of Texas Oil Spills

```{r}

oil_spills <- read_csv("oil_spills.csv")


df <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` < 2017) %>%  #filter the data to just be from Texas AND the year is less than 2017
  group_by(`Accident Year`) %>%   #Group the data by accident year.
  summarise(Loss = sum(`Net Loss (Barrels)`)) #Provide a summary of the net loss of barrels per group of years.


colnames(df) <- c("Year", "Loss")

ggplot(df, aes(x=Year, y=Loss))+
  geom_col()
```

Stage -> Commit -> Push

##Leaflet Plot of Spill Locations in Texas in 2016

```{r}

df_loc <- oil_spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` == 2016) %>% 
  select(Latitude, Longitude, `Net Loss (Barrels)`)


colnames(df_loc) <- c("latitude", "longitude", "net_loss")

oil_sf <- st_as_sf(df_loc, coords = c("longitude", "latitude"), crs = 4326) #convert a dataframe into a simplefeatures spatial data.  identify the long and lat in the datafram and setting the coordinate reference system (crs) to WGS (code is 4326)

leaflet(oil_sf) %>% 
  addTiles() %>% 
  addMarkers()

```

##Make a TMap plot with the Texas State shapefile

```{r}

states <- st_read(dsn = ".", layer = "states") #read in a shape file using simplefeatures (sf)

tex_border <- states %>% 
  filter(STATE_NAME == "Texas") %>% 
  st_transform(4326)


plot(tex_border)


tm_shape(tex_border)+
  tm_polygons()+
  tm_shape(oil_sf)+
  tm_dots(size = 0.3)

```

##Spatial Analysis
Are these points of oil spill spatially biased?

Convert the data into spatial points patterns.  This is a combination of the point data and the bounding window.


```{r}

spill_sp <- as(oil_sf, "Spatial") #converts from simple feature back to dataframe.

spill_ppp <- as(spill_sp, "ppp") #convert the Satial Dataframe that was just created into a point pattern p.


tx_sp <- as(tex_border, "Spatial") #convert the texas boarder to a spatial frame to make the window for analysis

tex_owin <- as(tx_sp, "owin") #conert the spatial frame of texas boarder and think of it as the outerwindow (owin)

all_ppp <- ppp(spill_ppp$x, spill_ppp$y, window = tex_owin)

```

###A Density Plot
Kernel density plots for overlapping areas become super easy and looks really dangerously awesome.

Takes point pattern

```{r}
plot(density(all_ppp, sigma = 0.4))


```

###Quadrat Test for Spatial Evenness

This does not test for randomness.  It tests to see if data is evenly distributed spatially.  Thats different from random!


```{r}
oil_qt <- quadrat.test(all_ppp, nx = 5, ny = 5) #nx is the number of "regions" you want horizantally, ny is the number of regions you want vertically.  ie the grids

oil_qt
```

This preforms a hypothesis test where the null hypothesis is that the data is CSR (or evenly distributed).  Since the p value is so low, we can reject the null and say "The Data is not evenly distributed!"



```{r}

plot(all_ppp)
plot(oil_qt, add = TRUE, cex = 0.4)
```

The numbers in the quadrat tells you:
1.  The number of expected events within each quadrat that you would expect if the data were fully even.For partial quadrats, it calculates the proportional number of counts expected for the incomplete quadrats based on area.
2.  The actual number of counts per region
3.  Standardized measure of how different the actual count is to the expected count.



###G-Function for Nearest Neighbor Analysis

```{r}
lag <- seq(0,1, by=0.01)

oil_gfun <- envelope(all_ppp, fun = Gest, r = lag, nsim = 100) #Find the nearest neighbor given increments of distance, our r.



```
The r column is the distance from each point.  The obs is the

```{r}
ggplot(oil_gfun, aes(x = r, y = obs))+ #the actual gfunction values from the oilspill data
  geom_line(color = "black")+
  geom_line(aes(x = r, y = theo), color = "red")
```



Our observed data has a higher proporiton of point pairs with nearest neighbors at shorter distances compared to csr data.  On average our data has nearest neighbors that are closer than we'd expect for randomly distributed data.

This is because the black part is above the red line. The black line is the observations, the red line is the "theoretical", or model driven version.


##Nearest Neighbor using the L-Function (Ripley's K standardized)
But what if we consider not just a single nearest neighbor, but rather consider a function of clusters of data.  OH BOY lets use the L-Function

```{r}

lag2 <- seq(0,3, by = 0.5) #this is a large increment for the sake of time in lab


oil_lfun <- envelope(all_ppp, fun = Lest, r = lag2, nsim = 20, global = TRUE) #this is not just finding a single nearest neighbor, but finds increasing bubbles around each point until it contains all observations.


ggplot(oil_lfun, aes(x = lag2, y = obs))+
  geom_line(color = "black")+
  geom_line(aes(x = lag2, y = theo), color = "blue")

```

OUr blue line is beneath our observation line indicating that the nearest neighbor is closer than farther away.  Same result as the previous one cause its the same data.


